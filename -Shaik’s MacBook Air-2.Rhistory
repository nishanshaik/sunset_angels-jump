### Load Libraries =================================
library(R0)
library(tidyverse)
library(zoo)
library(lubridate)
library(ggthemes)
library(ggdark)
library(scales)
### Custom functions =============================
## not in a vector
`%nin%` = Negate(`%in%`)
## Custom function for R0 estimation under gamma distributed generation times
get_R0 <- function(mean_gi = mean_gi, sd_gi = sd_gi, rho = rho) {
exp(((mean_gi ^ 2) * log(1 + ((sd_gi ^ 2) * rho / mean_gi))) / (sd_gi ^ 2))
}
## Setting working directory ======================
setwd("/Users/drshaiknishan/OneDrive/Documents/Website")
### Checkpoint ===================================
checkPoint <- "About to download data"
write.table(checkPoint, file = "CheckPoint1.txt", sep = "")
### Download Dataset ===============================
download.file(url = "https://api.covid19india.org/csv/latest/districts.csv",
destfile = "district.csv")
### Checkpoint ===================================
checkPoint <- "Download Complete"
write.table(checkPoint, file = "CheckPoint2.txt", sep = "")
### Load dataset ===================================
district <- read_csv("district.csv")
### Creating check variables from reliable final dataset =================
# reliable <- read_csv("Districts.csv") %>%
#   filter(State != "Placeholder") %>%
#   unite("check_list", c(State, District), sep = "_", remove = FALSE)
#
# write.csv(reliable, "check_list.csv", row.names = FALSE)
### Read in Check List ==============
# Note: CHecklist is a list of State_District pairs in a key format for those in the 633 districts that work without niggles. This step hopefully prevents future breakdown of the code due to punching typos fromcovid19india.org.
check <- read_csv("check_list.csv")
check_list <- check$check_list
### Correct Spellings ============================
# district$District[district$District == "Gamla"] <- "Gumla"
district <- district %>%
filter(District != "Gamla") %>%
filter(District != "Dibrugarh") %>%
filter(District != "Udalguri") %>%
filter(District != "Churachandpur") %>%
unite(c(State, District), col = "key", sep = "_", remove = FALSE) %>%
filter(key != "Himachal Pradesh_Dadra and Nagar Haveli") %>%
filter(key %in% check_list) %>%
select(-key)
## Change working directory =======================
setwd("/Users/drshaiknishan/OneDrive/Documents/Website/sunset_angels-jump")
### Serial Interval Estimates =====================
sample_si <- 3924
mean_si <- 5.17  # days
lci_si <- 4.89   # days
uci_si <- 5.45   # days
sd_si <- (mean_si - lci_si)/ 1.96
### Generation Interval ==========================
set.seed(2021)
gen_time <- generation.time(type = "gamma", val = c(mean_si, sd_si))
### Smoothing window for SMA =====================
smooth_duration <- 14
### Checkpoint ===================================
checkPoint <- "Starting Nation level"
write.table(checkPoint, file = "CheckPoint3.txt", sep = "")
### Time series for India ========================
# Preparing India Specific Dataset
india <- district %>%
group_by(Date) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE)) %>%
ungroup() %>%
mutate(Incidence = Confirmed - lag(Confirmed, order_by = Date)) %>%
dplyr::select(-Confirmed) %>%
filter(!is.na(Incidence))
est_len <- length(india$Date) - 1
# Estimating Rt for each day
set.seed(2021)
est_r0 <- est.R0.TD(epid = india$Incidence, gen_time, t = india$Date, begin = 1, end = est_len, nsim = 5000)
# Teasing out Rt values from the Rt object
Rt <- est_r0[["R"]]
Date <- as.Date(names(Rt))
names(Rt) <- NULL
# A tibble with Rt and dates
india <- as_tibble(data.frame(Date, Rt))
# Smoothed Rt (SMA) and final dataset
india <- india %>%
mutate(smR = rollmean(Rt, k = smooth_duration, align = "right", fill = NA)) %>%
filter(Date >= dmy("15-05-2020")) %>%
dplyr::select(Date, smR)
# Writing the dataset for website
write.csv(india, "India.csv", row.names = FALSE)
### Checkpoint ===================================
checkPoint <- "Nation done! Starting states"
write.table(checkPoint, file = "CheckPoint4.txt", sep = "")
### Time series for States ========================
# Preparing dataset for states
states <- district %>%
group_by(Date, State) %>%
summarise(Confirmed = sum(Confirmed, na.rm = TRUE)) %>%
ungroup() %>%
group_by(State) %>%
mutate(Incidence = Confirmed - lag(Confirmed, order_by = Date)) %>%
ungroup() %>%
dplyr::select(-Confirmed)
# Taking number of records for each state =============
x <- states %>%
filter(!is.na(Incidence))
table(x$State)
# Vector of state names ============================
state_names <- unique(states$State)
# Number of states included in the analysis ========
nStates <- length(state_names)
# Storage dataframe ========================
State <- "Placeholder"
Date <- as.Date("2016-10-10")
smR <- 1.00
perm_set <- as_tibble(data.frame(State = State, Date = Date, smR = smR))
# For loop for state-level analysis ==============
for(s in 1:nStates) {
# Dataset for one state
temp_state <- states %>%
filter(State == state_names[s]) %>%
filter(!is.na(Incidence))
# First day of case report for that state
first_outbreak <- min(temp_state$Date[temp_state$Incidence > 0])
temp_state <- temp_state %>%
filter(Date >= first_outbreak)
# Change all zeros to 1
temp_state$Incidence[temp_state$Incidence <= 0] <- 1
temp_state$Incidence[is.na(temp_state$Incidence)] <- 1
# Duration of Rt estimation
est_len <- length(temp_state$Date) - 1
# Rt estimation
a <- Sys.time()
set.seed(2021)
est_r0 <- est.R0.TD(epid = temp_state$Incidence, gen_time, t = temp_state$Date, begin = 1, end = est_len, nsim = 1500)
print(Sys.time() - a)
# Consolidating the numbers
Rt <- est_r0[["R"]]
Date <- as.Date(names(Rt))
names(Rt) <- NULL # removing metadata
# Preparing data frame
temp_set <- as_tibble(data.frame(Date, Rt))
temp_set <- temp_set %>%
mutate(smR = rollmean(Rt, k = smooth_duration, align = "right", fill = NA),
State = state_names[s]) %>%
dplyr::select(State, Date, smR)
perm_set <- bind_rows(perm_set, temp_set)
print(paste0("Estimation completed for ", state_names[s], ". ", s, " of ",  nStates, " completed!"))
}
# Preparing final dataset ===================
perm_set <- perm_set %>%
filter(!is.na(smR)) %>%
rename(Region = State,
Rt = smR)
# Write out CSV ====================
write.csv(perm_set, "States.csv", row.names = FALSE)
### Checkpoint ===================================
checkPoint <- "States complete, starting districts"
write.table(checkPoint, file = "CheckPoint5.txt", sep = "")
### Estimation for Districts ==============================
# Vector of noise in districts =====================
noise_districts <- c("Unknown", "Other State", "Other Region", "State Pool", "Italians", "Evacuees", "Railway Quarantine", "Foreign Evacuees", "Others", "Airport Quarantine", "Capital Complex")
# Prepare dataset ===================================
districts <- district %>%
filter(District %nin% noise_districts) %>%
unite("id", c(State, District), sep = "_", remove = FALSE) %>%
group_by(id) %>%
mutate(Incidence = Confirmed - lag(Confirmed, order_by = Date)) %>%
filter(!is.na(Incidence)) %>%
dplyr::select(id, State, District, Date, Incidence)
# Vector of state_district names ============================
district_names <- unique(districts$id)
# Number of states included in the analysis ========
nDistricts <- length(district_names)
# Storage dataframe ========================
State <- "Placeholder"
District <- "Placeholder"
Date <- as.Date("2016-10-10")
smR <- 1.00
dist_set <- as_tibble(data.frame(State = State, District = District, Date = Date, smR = smR))
# For loop for district-level analysis ==============
for(d in 1:nDistricts) {
begin_loop <- Sys.time()
# Dataset for one district
temp_district <- districts %>%
filter(id == district_names[d]) %>%
filter(!is.na(Incidence))
# State and District names
sName <- temp_district$State[1]
dName <- temp_district$District[1]
# First day of case report for that district
first_outbreak <- min(temp_district$Date[temp_district$Incidence > 0])
temp_district <- temp_district %>%
filter(Date >= first_outbreak)
# Change all zeros to 1
temp_district$Incidence[temp_district$Incidence <= 0] <- 1
temp_district$Incidence[is.na(temp_district$Incidence)] <- 1
# Duration of Rt estimation
est_len <- length(temp_district$Date) - 1
set.seed(2021)
# Rt estimation
a <- Sys.time()
est_r0 <- est.R0.TD(epid = temp_district$Incidence, gen_time, t = temp_district$Date, begin = 1, end = est_len, nsim = 1000)
print(Sys.time() - a)
# Consolidating the numbers
Rt <- est_r0[["R"]]
Date <- as.Date(names(Rt))
names(Rt) <- NULL # removing metadata
# Preparing data frame
temp_set <- as_tibble(data.frame(Date, Rt))
temp_set <- temp_set %>%
mutate(smR = rollmean(Rt, k = smooth_duration, align = "right", fill = NA),
State = sName,
District = dName) %>%
dplyr::select(State, District, Date, smR)
dist_set <- bind_rows(dist_set, temp_set)
print(paste0("Estimation completed for ", dName, " in ", sName, ". ", d, " of ",  nDistricts, " completed!"))
print(Sys.time() - begin_loop)
}
# Preparing final dataset ===================
dist_set <- dist_set %>%
filter(!is.na(smR))
# Write out CSV ====================
write.csv(dist_set, "Districts.csv", row.names = FALSE)
### Checkpoint ===================================
checkPoint <- "States complete. Proceeding to render website"
write.table(checkPoint, file = "CheckPoint6.txt", sep = "")
